{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet\n",
    "from autocorrect import Speller\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doc2Vec(table_name: str):\n",
    "    return encoder.encode(df[table_name])\n",
    "\n",
    "pf = ParquetFile(\"data/exmp_rus.parquet\")\n",
    "a = next(pf.iter_batches(batch_size=1000))\n",
    "df = pa.Table.from_batches([a]).to_pandas()\n",
    "\n",
    "np.save(\"table_channel_title\", df['channel_title']) \n",
    "np.save(\"table_video_title\", df['video_title'])\n",
    "np.save(\"table_video_id\", df['video_id'])\n",
    "\n",
    "encoder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "index_arr = []\n",
    "for i, el in enumerate([Doc2Vec('video_title'), Doc2Vec('channel_title')]):\n",
    "    index_arr.append(faiss.IndexFlatL2(el.shape[1]))\n",
    "    faiss.normalize_L2(el)\n",
    "    index_arr[i].add(el)\n",
    "np.save(\"index_arr\", index_arr) # save indexes to not count every time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a selection of names from a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initialization started\")\n",
    "encoder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "index_arr = np.load('index_arr.npy', allow_pickle=True)\n",
    "table_t = np.load('table_video_title.npy', allow_pickle=True)\n",
    "table_i = np.load('table_video_id.npy', allow_pickle=True)\n",
    "table_c = np.load('table_channel_title.npy', allow_pickle=True)\n",
    "spell = Speller('ru')\n",
    "\n",
    "print(\"Finding started\")\n",
    "search_text = spell('Рыбалкв')\n",
    "search_vector = encoder.encode(search_text)\n",
    "_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(_vector)\n",
    "\n",
    "ann_arr = []\n",
    "for i in range(len(index_arr)):\n",
    "    ann_arr.append(index_arr[i].search(_vector, k=index_arr[i].ntotal))\n",
    "\n",
    "for i in range(5):\n",
    "    if ann_arr[0][0][0][i] <= ann_arr[1][0][0][i] * np.pi: # trying to suggest also name of channels\n",
    "        print(table_i[ann_arr[0][1][0][i]] + \" --- \" + table_c[ann_arr[0][1][0][i]] + \" --- \" + table_t[ann_arr[0][1][0][i]])\n",
    "    else:\n",
    "        print(table_i[ann_arr[1][1][0][i]] + \" --- \" + table_c[ann_arr[1][1][0][i]] + \" --- \" + table_t[ann_arr[1][1][0][i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
